{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0b31749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading Environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68582937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Dependencies\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain.tools import tool\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.agents import create_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50ca7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define documents categories and file mapping\n",
    "files = [\n",
    "    (\"CaseStudy_1.pdf\", \"CaseStudy\"),\n",
    "    (\"CaseStudy_2.pdf\", \"CaseStudy\"),\n",
    "    (\"CaseStudy_3.pdf\", \"CaseStudy\"),\n",
    "    (\"faqs_1.pdf\", \"faq\"),\n",
    "    (\"faqs_2.pdf\", \"faq\"),\n",
    "    (\"Newsletter_1.pdf\", \"Newsletter\"),\n",
    "    (\"Newsletter_2.pdf\", \"Newsletter\"),\n",
    "    (\"Newsletter_3.pdf\", \"Newsletter\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87a79ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare Chroma directories per category\n",
    "base_dir = \"./chroma_agentic\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "embedding_model = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e08ee55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: CaseStudy_1.pdf\n",
      "Processing: CaseStudy_2.pdf\n",
      "Processing: CaseStudy_3.pdf\n",
      "Processing: faqs_1.pdf\n",
      "Processing: faqs_2.pdf\n",
      "Processing: Newsletter_1.pdf\n",
      "Processing: Newsletter_2.pdf\n",
      "Processing: Newsletter_3.pdf\n"
     ]
    }
   ],
   "source": [
    "# 5. Split and create category-based vector stores\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "category_docs = {\"CaseStudy\": [], \"faq\": [], \"Newsletter\": []}\n",
    "\n",
    "for file_path, category in files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        page.metadata[\"category\"] = category\n",
    "    chunks = splitter.split_documents(pages)\n",
    "    category_docs[category].extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0de31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading existing vector store for CaseStudy...\n",
      "üîÅ Loading existing vector store for faq...\n",
      "üîÅ Loading existing vector store for Newsletter...\n",
      "\n",
      "‚úÖ Vector stores ready for all categories!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Create or load Chroma DB for each category\n",
    "vectorstores = {}\n",
    "for category, docs in category_docs.items():\n",
    "    persist_dir = os.path.join(base_dir, category.lower())\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    if len(os.listdir(persist_dir)) == 0:\n",
    "        print(f\"üß† Creating new vector store for {category}...\")\n",
    "        vs = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "    else:\n",
    "        print(f\"üîÅ Loading existing vector store for {category}...\")\n",
    "        vs = Chroma(embedding_function=embedding_model, persist_directory=persist_dir)\n",
    "\n",
    "    vectorstores[category] = vs\n",
    "\n",
    "print(\"\\n‚úÖ Vector stores ready for all categories!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b90a055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Agentic category selector\n",
    "category_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "You are an intelligent assistant. Based on the user's question, decide which category of company documents\n",
    "the answer is most likely found in. \n",
    "\n",
    "Choose only one category from: \n",
    "1. faq \n",
    "2. CaseStudy \n",
    "3. Newsletter\n",
    "\n",
    "Question: {question}\n",
    "Category:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def choose_category(question):\n",
    "    \"\"\"Ask the LLM to classify query into a document category\"\"\"\n",
    "    response = llm.invoke(category_prompt.format(question=question))\n",
    "    result = response.content.strip()\n",
    "    if \"case\" in result.lower():\n",
    "        return \"CaseStudy\"\n",
    "    elif \"faq\" in result.lower():\n",
    "        return \"faq\"\n",
    "    elif \"news\" in result.lower():\n",
    "        return \"Newsletter\"\n",
    "    else:\n",
    "        return \"faq\"  # Default fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2748cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Build the retrieval QA chain\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a company assistant. Use the provided context to answer the user's question clearly and concisely.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdb3570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_response(question):\n",
    "    \"\"\"Complete RAG pipeline with dynamic category routing\"\"\"\n",
    "    category = choose_category(question)\n",
    "    print(f\"\\nü§ñ Chosen Category: {category}\")\n",
    "\n",
    "    vectorstore = vectorstores[category]\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "    response = retriever.invoke({\"query\": question})\n",
    "    print(\"\\n Response:\\n\", response[\"result\"])\n",
    "\n",
    "    print(\"\\nüìö Sources:\")\n",
    "    for doc in response[\"source_documents\"]:\n",
    "        print(f\" - {doc.metadata.get('source')} | Category: {doc.metadata.get('category')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f78f4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    model_provider=\"groq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b17cd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_agent(llm, [choose_category])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
